{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12479334,"sourceType":"datasetVersion","datasetId":7873971}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install huggingface_hub\n\n","metadata":{"id":"JtHzndd2ejh5","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T08:56:33.653298Z","iopub.execute_input":"2025-07-16T08:56:33.653554Z","iopub.status.idle":"2025-07-16T08:56:39.279494Z","shell.execute_reply.started":"2025-07-16T08:56:33.653535Z","shell.execute_reply":"2025-07-16T08:56:39.278230Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.5.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.4)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.6.15)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import login\nimport os\n\n# Access secret\nhf_token = os.environ.get(\"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\")\n\n# Login using token from secret\nlogin(token=hf_token)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T08:56:41.717864Z","iopub.execute_input":"2025-07-16T08:56:41.718420Z","iopub.status.idle":"2025-07-16T08:56:42.408042Z","shell.execute_reply.started":"2025-07-16T08:56:41.718371Z","shell.execute_reply":"2025-07-16T08:56:42.406845Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0c284d3607844d58562531a5436d42b"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"# No login needed for public models\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\nmodel = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-16T08:56:58.381658Z","iopub.execute_input":"2025-07-16T08:56:58.382023Z","iopub.status.idle":"2025-07-16T08:57:46.617532Z","shell.execute_reply.started":"2025-07-16T08:56:58.381958Z","shell.execute_reply":"2025-07-16T08:57:46.616092Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a50300882f5646729dc7ecda231fb23a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"036e013b20da4afc9b346cfc49146462"}},"metadata":{}},{"name":"stderr","text":"2025-07-16 08:57:24.970466: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752656245.241838      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752656245.315148      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f23a97c9ef7b46bf8a9f1995947c4da8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/442M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee19f2fa87604bdf8d4628195164f953"}},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nfile_path = \"/kaggle/input/students-gpt-responses-category-6/Students_gpt_Responses _Category_6.xlsx\"\ndataset = pd.read_excel(file_path)\ndf = dataset\nprint(f\"Number of rows: {len(df)}\")\ndataset.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"id":"_355rnjsev1R","outputId":"69f4777e-227f-4376-a194-dd2b5c1c024c","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T08:59:55.530416Z","iopub.execute_input":"2025-07-16T08:59:55.531887Z","iopub.status.idle":"2025-07-16T08:59:56.580687Z","shell.execute_reply.started":"2025-07-16T08:59:55.531847Z","shell.execute_reply":"2025-07-16T08:59:56.579739Z"}},"outputs":[{"name":"stdout","text":"Number of rows: 1617\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"      ID     Type                                          Responses  \\\n0  N1058  Student  The carts would go the opposite direction from...   \n1  N1069  Student  At the proximity illustrated, their electric f...   \n2  N1136  Student  The cars will go in the opposite direction bec...   \n3  N1141  Student  When two negative charges are close together t...   \n4  N1162  Student  They will move in the opposite direction becau...   \n\n   Category 6  \n0           1  \n1           1  \n2           1  \n3           1  \n4           1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Type</th>\n      <th>Responses</th>\n      <th>Category 6</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>N1058</td>\n      <td>Student</td>\n      <td>The carts would go the opposite direction from...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>N1069</td>\n      <td>Student</td>\n      <td>At the proximity illustrated, their electric f...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>N1136</td>\n      <td>Student</td>\n      <td>The cars will go in the opposite direction bec...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>N1141</td>\n      <td>Student</td>\n      <td>When two negative charges are close together t...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>N1162</td>\n      <td>Student</td>\n      <td>They will move in the opposite direction becau...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\n\n# -------- Data Extraction --------\nX_texts = df[\"Responses\"].fillna(\"\").astype(str).tolist()\nY = df[\"Category 6\"].apply(pd.to_numeric, errors='coerce').fillna(0).astype(int).values\n\n# -------- Load SciBERT --------\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\nmodel = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\nmodel.eval()\nfor param in model.parameters():\n    param.requires_grad = False\n\n# -------- Encode with SciBERT --------\ndef get_bert_embeddings(texts, tokenizer, model, max_length=128):\n    all_embeddings = []\n    with torch.no_grad():\n        for text in tqdm(texts, desc=\"Encoding texts with SciBERT\"):\n            if not isinstance(text, str):\n                text = str(text) if text is not None else \"\"\n            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=max_length)\n            outputs = model(**inputs)\n            cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n            all_embeddings.append(cls_embedding)\n    return np.array(all_embeddings)\n\nX_embeddings = get_bert_embeddings(X_texts, tokenizer, model)\n\n# -------- Train/Test split --------\nX_train, X_test, Y_train, Y_test = train_test_split(X_embeddings, Y, test_size=0.2, random_state=42)\n\n# -------- Classification --------\n#clf = MultiOutputClassifier(LogisticRegression(max_iter=1000))\nclf = LogisticRegression(max_iter=1000)\nclf.fit(X_train, Y_train)\nY_pred = clf.predict(X_test)\n\n# -------- Evaluation --------\nresults = []\n\n\n# -------- Evaluation (single label) --------\nacc = accuracy_score(Y_test, Y_pred)\nprec = precision_score(Y_test, Y_pred, zero_division=0)\nrec = recall_score(Y_test, Y_pred, zero_division=0)\nf1 = f1_score(Y_test, Y_pred, zero_division=0)\n\nprint(\"Evaluation Results for 'Category 6':\")\nprint(f\"Accuracy:  {acc:.4f}\")\nprint(f\"Precision: {prec:.4f}\")\nprint(f\"Recall:    {rec:.4f}\")\nprint(f\"F1 Score:  {f1:.4f}\")\n\n\n\n\n#for i, cat in enumerate(category_cols):\n #   unique_classes = np.unique(Y_test[:, i])\n  #  average_type = 'binary' if (len(unique_classes) == 2 and set(unique_classes) == {0, 1}) else 'macro'\n\n   # acc = accuracy_score(Y_test[:, i], Y_pred[:, i])\n    #prec = precision_score(Y_test[:, i], Y_pred[:, i], zero_division=0, average=average_type)\n    #rec = recall_score(Y_test[:, i], Y_pred[:, i], zero_division=0, average=average_type)\n    #f1 = f1_score(Y_test[:, i], Y_pred[:, i], zero_division=0, average=average_type)\n    #results.append([cat, acc, prec, rec, f1])\n\n#results_df = pd.DataFrame(results, columns=[\"Category\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"])\n#print(results_df)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QwgFU-pPg08J","outputId":"ffde45e1-b7f0-4e4f-e8ee-e5750f92a649","trusted":true,"execution":{"iopub.status.busy":"2025-07-15T17:00:58.047294Z","iopub.execute_input":"2025-07-15T17:00:58.049115Z","iopub.status.idle":"2025-07-15T17:07:23.086726Z","shell.execute_reply.started":"2025-07-15T17:00:58.049063Z","shell.execute_reply":"2025-07-15T17:07:23.085761Z"}},"outputs":[{"name":"stderr","text":"Encoding texts with SciBERT: 100%|██████████| 1617/1617 [06:21<00:00,  4.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Evaluation Results for 'Category 6':\nAccuracy:  0.9846\nPrecision: 0.8966\nRecall:    0.9286\nF1 Score:  0.9123\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom tqdm import tqdm\n\nX_texts = df[\"Responses\"].fillna(\"\").astype(str).tolist()\nY = df[\"Category 6\"].apply(pd.to_numeric, errors='coerce').fillna(0).astype(int).values\n\nX_train, X_val, Y_train, Y_val = train_test_split(X_texts, Y, test_size=0.2, random_state=42)\n\n# ----------------------- Custom Dataset -----------------------\nclass MultiLabelDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt')\n        item = {key: val.squeeze(0) for key, val in encoding.items()}\n        item[\"labels\"] = torch.tensor([label], dtype=torch.float)\n\n        #item[\"labels\"] = torch.tensor(label, dtype=torch.float)\n        return item\n\n# ----------------------- Load SciBERT -----------------------\nmodel_name = \"allenai/scibert_scivocab_uncased\"\n\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=1,\n    #num_labels=len(category_cols),\n    problem_type=\"multi_label_classification\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# ----------------------- Dataloaders -----------------------\nbatch_size = 16\n\ntrain_dataset = MultiLabelDataset(X_train, Y_train, tokenizer)\nval_dataset = MultiLabelDataset(X_val, Y_val, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\n\n# ----------------------- Optimizer & Loss -----------------------\noptimizer = AdamW(model.parameters(), lr=2e-5)\nloss_fn = torch.nn.BCEWithLogitsLoss()\n\n# ----------------------- Training Loop -----------------------\nepochs = 10\nfor epoch in range(epochs):\n    model.train()\n    train_loss = 0\n    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n\n    for batch in loop:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        loss = loss_fn(logits, labels)\n        train_loss += loss.item()\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        loop.set_postfix(loss=loss.item())\n\n    print(f\"Epoch {epoch+1} - Train Loss: {train_loss / len(train_loader):.4f}\")\n\n# ----------------------- Evaluation -----------------------\nmodel.eval()\nall_preds = []\nall_probs = []\nall_labels = []\n\nwith torch.no_grad():\n    for batch in tqdm(val_loader, desc=\"Evaluating\"):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        probs = torch.sigmoid(logits)\n        preds = (probs > 0.5).int()\n\n        all_preds.append(preds.cpu())\n        all_probs.append(probs.cpu())\n        all_labels.append(labels.cpu())\n\nY_pred = torch.cat(all_preds).numpy()\nY_prob = torch.cat(all_probs).numpy()\nY_true = torch.cat(all_labels).numpy()\n\n# ----------------------- Metrics -----------------------\n\n\n# -------- Evaluation (single label) --------\nacc = accuracy_score(Y_true, Y_pred)\nprec = precision_score(Y_true, Y_pred, zero_division=0)\nrec = recall_score(Y_true, Y_pred, zero_division=0)\nf1 = f1_score(Y_true, Y_pred, zero_division=0)\n\nprint(\"Evaluation Results for 'Category 6':\")\nprint(f\"Accuracy:  {acc:.4f}\")\nprint(f\"Precision: {prec:.4f}\")\nprint(f\"Recall:    {rec:.4f}\")\nprint(f\"F1 Score:  {f1:.4f}\")\n\n\n","metadata":{"id":"vhixMA_G4rYr","trusted":true,"execution":{"iopub.status.busy":"2025-07-16T09:00:08.502575Z","iopub.execute_input":"2025-07-16T09:00:08.503310Z","iopub.status.idle":"2025-07-16T20:40:57.362154Z","shell.execute_reply.started":"2025-07-16T09:00:08.503278Z","shell.execute_reply":"2025-07-16T20:40:57.360073Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Train Loss: 0.1406\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Train Loss: 0.0505\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Train Loss: 0.0226\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Train Loss: 0.0098\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Train Loss: 0.0067\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6 - Train Loss: 0.0171\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7 - Train Loss: 0.0215\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8 - Train Loss: 0.0040\n","output_type":"stream"},{"name":"stderr","text":"                                                                            \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9 - Train Loss: 0.0006\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10 - Train Loss: 0.0004\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|██████████| 21/21 [04:34<00:00, 13.06s/it]","output_type":"stream"},{"name":"stdout","text":"Evaluation Results for 'Category 6':\nAccuracy:  0.9907\nPrecision: 0.9310\nRecall:    0.9643\nF1 Score:  0.9474\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5}]}