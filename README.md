# Exploring Data Augmentation and Resampling Strategies for Transformer-Based Models to Address Class Imbalance in AI Scoring of Scientific Explanations in NGSS Classroom
Optimizing-AI-Scoring-of-Scientific-Explanations-Exploring-Augmentation-Strategies-

Automated scoring of students’ scientific explanations offers the potential for immediate, accurate feedback, yet class imbalance in rubric categories—particularly those capturing advanced reasoning—remains a challenge. This study investigates augmentation strategies to improve transformer-based text classification of student responses to a physical science assessment based on an NGSS-aligned learning progression. The dataset consists of 1,466 high school responses scored on 11 binary-coded analytic categories. This rubric identifies six scientific ideas used for a complete explanation along with five common incomplete or inaccurate ideas. Using SciBERT as a baseline, we applied fine-tuning and test these augmentation strategies: (1) GPT-4–generated synthetic responses, and (2) EASE, a word-level extraction and filtering approach, (3) ALP (Augmentation using Lexicalized Probabilistic context-free grammar) phrase-level extraction. 

While fine-tuning SciBERT improved recall over baseline, augmentation substantially enhanced performance, with GPT data boosting both precision and recall, and ALP achieving perfect precision, recall, and F1 scores across most severe imbalanced categories (5,6,7 and 9). Across all rubric categories EASE augmentation substantially increased alignment with human scoring for both scientific ideas (Categories 1–6) and inaccurate ideas (Categories 7–11). We compared different augmentation strategies to a traditional oversampling method (SMOTE) in an effort to avoid overfitting and retain novice-level data critical for learning progression alignment. Findings demonstrate that targeted augmentation can address severe imbalance while preserving conceptual coverage, offering a scalable solution for automated learning progression-aligned scoring in science education. 


# Implications

This work contributes to advancing automated scoring of student explanations including complex student reasoning using AI tools. Our study targets a pressing need in science assessment, the automated classification of open-ended responses using analytic rubrics that reflect learning progressions, which reflect various sophistication of student understanding while learning. The study examined which augmentation strategies most effectively mitigate class imbalance in transformer-based automated scoring of student scientific explanations aligned with a learning progression (LP). Using SciBERT as the base classifier, we compared a no-augmentation baseline and fine-tuned model against resampling (SMOTE) and three data-space augmentation approaches (GPT-generated document-level augmentation, EASE word-level augmentation, and ALP phrase-level augmentation). 

Our results show that targeted data augmentation is the most reliable solution for severe imbalance, especially in the most conceptually advanced and rare categories (e.g., Categories 5–6, which represent higher-level energy and Coulombic reasoning and are among the most imbalanced and instructionally critical for LP placement). In the extreme-imbalance setting, fine-tuning alone improved recall but still left important gaps, whereas GPT augmentation improved both precision and recall, and ALP delivered the strongest performance, achieving perfect precision/recall/F1 in the most skewed categories (Categories 5, 6,7 and 9). 

By focusing on imbalanced but conceptually rich categories like reasoning about energy and Coulombic interactions (Categories 5 and 6), we respond to a common issue in classroom data: small datasets skewed toward novice reasoning. These data limitations often undermine efforts to validly assess deeper levels of scientific understanding, including the use of LP-aligned assessments. Our integration of SciBERT with augmentation strategies (specifically, ALP and EASE) yielded high classification performance even under conditions of severe class imbalance. These results represent a methodological advance for educators and researchers seeking scalable approaches to using three dimensional assessments aligned to LPs.  Importantly, these categories play a central role for assigning students to advanced levels of the LP, underscoring the importance of accurate scoring of these categories. 

Notably, the same augmentation strategies also improved predictive performance for Categories 7 and 9.  Although these categories also occur infrequently, they capture consequential inaccuracies in student reasoning: one reflects conflation of magnetic and electric forces, while the other indicates a misunderstanding of Coulombic interactions in the item context. Because these inaccurate ideas are lexically and linguistically similar to parts of normative scientific explanations, they are particularly difficult to detect using automated scoring approaches.  Yet identifying these ideas is essential, as advancing along the LP requires that such inaccurate ideas be explicitly addressed through student revision and instructional support.  The observed improvements in model performance with ALP and EASE therefore suggest that these augmentation strategies may be broadly applicable for identifying a wide range of targeted, conceptually nuanced ideas, both normative and non-normative, in student explanations.

This is further supported by examination of the average performance of different strategies across all categories, exhibiting a range of balance. Across all rubric categories, augmentation strategies, particularly EASE, substantially increased alignment with human scoring for both scientific ideas (Categories 1–6) and inaccurate ideas (Categories 7–11), yielding near maximal performance relative to the baseline and fine-tuning alone. ALP augmentation exhibited similar performance as EASE on both scientific and inaccurate ideas, however exhibited lower recall.  This is critical since it means ALP is missing more positive cases of some of these inaccurate ideas, so responses that display such inaccuracies may not be properly labeled.  Nevertheless, the EASE and ALP overall performance was strong. Moreover, because these augmentation strategies can be implemented automatically within a text classification pipeline, they reduce reliance on additional data collection or manual labeling of student responses.

 A key implication is that resampling alone is not sufficient for LP-aligned automated scoring under extreme imbalance. While resampling can increase minority-class exposure, it can also introduce artifacts: oversampling may encourage memorization when few unique minority examples exist, and undersampling can remove valuable novice-level responses needed to preserve conceptual coverage along the LP. Consistent with this limitation, SMOTE underperformed relative to augmentation in our aggregate results, plausibly due to overfitting and reduced linguistic/semantic diversity when balancing is achieved primarily through feature-space interpolation rather than generating meaning-preserving variations in students’ natural language. 

For practice and future development, these findings suggest that augmentation-centered pipelines EASE and ALP can make LP-aligned scoring more scalable and instructionally useful: they improve detection of rare but critical forms of sophisticated reasoning (Categories 5–6) and also strengthen identification of subtle misconceptions (e.g., misinterpreting Coulomb’s law), which are essential targets for formative feedback and revision. At the same time, the use of synthetic data motivates careful validity work: future studies should test generalization across additional items, rubrics, domains, and student populations and incorporate systematic quality checks by experts to ensure that augmented responses preserve the intended construct.





