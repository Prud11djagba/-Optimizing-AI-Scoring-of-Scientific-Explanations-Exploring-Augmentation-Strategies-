{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12479334,"sourceType":"datasetVersion","datasetId":7873971},{"sourceId":12499529,"sourceType":"datasetVersion","datasetId":7888688}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install huggingface_hub\n\n","metadata":{"id":"JtHzndd2ejh5","trusted":true,"execution":{"iopub.status.busy":"2025-07-18T08:42:22.031318Z","iopub.execute_input":"2025-07-18T08:42:22.032165Z","iopub.status.idle":"2025-07-18T08:42:26.560027Z","shell.execute_reply.started":"2025-07-18T08:42:22.032134Z","shell.execute_reply":"2025-07-18T08:42:26.558859Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.33.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.5.1)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.4)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.5)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.6.15)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from huggingface_hub import login\nimport os\n\n# Access secret\nhf_token = os.environ.get(\"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\")\n\n# Login using token from secret\nlogin(token=hf_token)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T08:42:30.555466Z","iopub.execute_input":"2025-07-18T08:42:30.555851Z","iopub.status.idle":"2025-07-18T08:42:30.576211Z","shell.execute_reply.started":"2025-07-18T08:42:30.555817Z","shell.execute_reply":"2025-07-18T08:42:30.575106Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43e73036dfcb4b409d0ea535d42bc4fe"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# No login needed for public models\nfrom transformers import AutoTokenizer, AutoModel\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\nmodel = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T08:43:14.787497Z","iopub.execute_input":"2025-07-18T08:43:14.788249Z","iopub.status.idle":"2025-07-18T08:43:16.553008Z","shell.execute_reply.started":"2025-07-18T08:43:14.788212Z","shell.execute_reply":"2025-07-18T08:43:16.552042Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import pandas as pd\nfile_path = \"/kaggle/input/students-gpt-responses-category-5/Students_gpt_Responses _Category_5.xlsx\"\ndataset = pd.read_excel(file_path)\ndf = dataset\nprint(f\"Number of rows: {len(df)}\")\ndataset.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":224},"id":"_355rnjsev1R","outputId":"69f4777e-227f-4376-a194-dd2b5c1c024c","trusted":true,"execution":{"iopub.status.busy":"2025-07-18T08:43:10.191171Z","iopub.execute_input":"2025-07-18T08:43:10.191509Z","iopub.status.idle":"2025-07-18T08:43:11.087414Z","shell.execute_reply.started":"2025-07-18T08:43:10.191485Z","shell.execute_reply":"2025-07-18T08:43:11.086389Z"}},"outputs":[{"name":"stdout","text":"Number of rows: 1552\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"      ID     Type                                          Responses  \\\n0  N1047  Student  they have the same charge and they will reple ...   \n1  N1081  Student  If the wedges were to be removed the two cars ...   \n2  N1096  Student  The cars will move away from each other becaus...   \n3  N1161  Student  The cars will move away from each other becaus...   \n4  N1255  Student  The carts would definitely move opposite direc...   \n\n   Category 5  \n0           1  \n1           1  \n2           1  \n3           1  \n4           1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>Type</th>\n      <th>Responses</th>\n      <th>Category 5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>N1047</td>\n      <td>Student</td>\n      <td>they have the same charge and they will reple ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>N1081</td>\n      <td>Student</td>\n      <td>If the wedges were to be removed the two cars ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>N1096</td>\n      <td>Student</td>\n      <td>The cars will move away from each other becaus...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>N1161</td>\n      <td>Student</td>\n      <td>The cars will move away from each other becaus...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>N1255</td>\n      <td>Student</td>\n      <td>The carts would definitely move opposite direc...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModel\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom tqdm import tqdm\n\n# -------- Data Extraction --------\nX_texts = df[\"Responses\"].fillna(\"\").astype(str).tolist()\nY = df[\"Category 5\"].apply(pd.to_numeric, errors='coerce').fillna(0).astype(int).values\n\n# -------- Load SciBERT --------\ntokenizer = AutoTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\")\nmodel = AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\nmodel.eval()\nfor param in model.parameters():\n    param.requires_grad = False\n\n# -------- Encode with SciBERT --------\ndef get_bert_embeddings(texts, tokenizer, model, max_length=128):\n    all_embeddings = []\n    with torch.no_grad():\n        for text in tqdm(texts, desc=\"Encoding texts with SciBERT\"):\n            if not isinstance(text, str):\n                text = str(text) if text is not None else \"\"\n            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=max_length)\n            outputs = model(**inputs)\n            cls_embedding = outputs.last_hidden_state[:, 0, :].squeeze().numpy()\n            all_embeddings.append(cls_embedding)\n    return np.array(all_embeddings)\n\nX_embeddings = get_bert_embeddings(X_texts, tokenizer, model)\n\n# -------- Train/Test split --------\nX_train, X_test, Y_train, Y_test = train_test_split(X_embeddings, Y, test_size=0.2, random_state=42)\n\n# -------- Classification --------\n#clf = MultiOutputClassifier(LogisticRegression(max_iter=1000))\nclf = LogisticRegression(max_iter=1000)\nclf.fit(X_train, Y_train)\nY_pred = clf.predict(X_test)\n\n# -------- Evaluation --------\nresults = []\n\n\n# -------- Evaluation (single label) --------\nacc = accuracy_score(Y_test, Y_pred)\nprec = precision_score(Y_test, Y_pred, zero_division=0)\nrec = recall_score(Y_test, Y_pred, zero_division=0)\nf1 = f1_score(Y_test, Y_pred, zero_division=0)\n\nprint(\"Evaluation Results for 'Category 5':\")\nprint(f\"Accuracy:  {acc:.4f}\")\nprint(f\"Precision: {prec:.4f}\")\nprint(f\"Recall:    {rec:.4f}\")\nprint(f\"F1 Score:  {f1:.4f}\")\n\n\n\n\n#for i, cat in enumerate(category_cols):\n #   unique_classes = np.unique(Y_test[:, i])\n  #  average_type = 'binary' if (len(unique_classes) == 2 and set(unique_classes) == {0, 1}) else 'macro'\n\n   # acc = accuracy_score(Y_test[:, i], Y_pred[:, i])\n    #prec = precision_score(Y_test[:, i], Y_pred[:, i], zero_division=0, average=average_type)\n    #rec = recall_score(Y_test[:, i], Y_pred[:, i], zero_division=0, average=average_type)\n    #f1 = f1_score(Y_test[:, i], Y_pred[:, i], zero_division=0, average=average_type)\n    #results.append([cat, acc, prec, rec, f1])\n\n#results_df = pd.DataFrame(results, columns=[\"Category\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"])\n#print(results_df)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QwgFU-pPg08J","outputId":"ffde45e1-b7f0-4e4f-e8ee-e5750f92a649","trusted":true,"execution":{"iopub.status.busy":"2025-07-17T14:54:24.771479Z","iopub.execute_input":"2025-07-17T14:54:24.771975Z","iopub.status.idle":"2025-07-17T15:01:52.213391Z","shell.execute_reply.started":"2025-07-17T14:54:24.771938Z","shell.execute_reply":"2025-07-17T15:01:52.212667Z"}},"outputs":[{"name":"stderr","text":"Encoding texts with SciBERT: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1552/1552 [07:23<00:00,  3.50it/s]\n","output_type":"stream"},{"name":"stdout","text":"Evaluation Results for 'Category 5':\nAccuracy:  0.9775\nPrecision: 0.9583\nRecall:    0.7931\nF1 Score:  0.8679\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom tqdm import tqdm\n\nX_texts = df[\"Responses\"].fillna(\"\").astype(str).tolist()\nY = df[\"Category 5\"].apply(pd.to_numeric, errors='coerce').fillna(0).astype(int).values\n\nX_train, X_val, Y_train, Y_val = train_test_split(X_texts, Y, test_size=0.2, random_state=42)\n\n# ----------------------- Custom Dataset -----------------------\nclass MultiLabelDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_length=512):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n\n        encoding = self.tokenizer(\n            text,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_length,\n            return_tensors='pt')\n        item = {key: val.squeeze(0) for key, val in encoding.items()}\n        item[\"labels\"] = torch.tensor([label], dtype=torch.float)\n\n        #item[\"labels\"] = torch.tensor(label, dtype=torch.float)\n        return item\n\n# ----------------------- Load SciBERT -----------------------\nmodel_name = \"allenai/scibert_scivocab_uncased\"\n\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=1,\n    #num_labels=len(category_cols),\n    problem_type=\"multi_label_classification\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# ----------------------- Dataloaders -----------------------\nbatch_size = 16\n\ntrain_dataset = MultiLabelDataset(X_train, Y_train, tokenizer)\nval_dataset = MultiLabelDataset(X_val, Y_val, tokenizer)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size)\n\n# ----------------------- Optimizer & Loss -----------------------\noptimizer = AdamW(model.parameters(), lr=2e-5)\nloss_fn = torch.nn.BCEWithLogitsLoss()\n\n# ----------------------- Training Loop -----------------------\nepochs = 10\nfor epoch in range(epochs):\n    model.train()\n    train_loss = 0\n    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n\n    for batch in loop:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        loss = loss_fn(logits, labels)\n        train_loss += loss.item()\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        loop.set_postfix(loss=loss.item())\n\n    print(f\"Epoch {epoch+1} - Train Loss: {train_loss / len(train_loader):.4f}\")\n\n# ----------------------- Evaluation -----------------------\nmodel.eval()\nall_preds = []\nall_probs = []\nall_labels = []\n\nwith torch.no_grad():\n    for batch in tqdm(val_loader, desc=\"Evaluating\"):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"labels\"].to(device)\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n        probs = torch.sigmoid(logits)\n        preds = (probs > 0.5).int()\n\n        all_preds.append(preds.cpu())\n        all_probs.append(probs.cpu())\n        all_labels.append(labels.cpu())\n\nY_pred = torch.cat(all_preds).numpy()\nY_prob = torch.cat(all_probs).numpy()\nY_true = torch.cat(all_labels).numpy()\n\n# ----------------------- Metrics -----------------------\n\n\n# -------- Evaluation (single label) --------\nacc = accuracy_score(Y_true, Y_pred)\nprec = precision_score(Y_true, Y_pred, zero_division=0)\nrec = recall_score(Y_true, Y_pred, zero_division=0)\nf1 = f1_score(Y_true, Y_pred, zero_division=0)\n\nprint(\"Evaluation Results for 'Category 5':\")\nprint(f\"Accuracy:  {acc:.4f}\")\nprint(f\"Precision: {prec:.4f}\")\nprint(f\"Recall:    {rec:.4f}\")\nprint(f\"F1 Score:  {f1:.4f}\")\n\n\n","metadata":{"id":"vhixMA_G4rYr","trusted":true,"execution":{"iopub.status.busy":"2025-07-18T08:43:38.932405Z","iopub.execute_input":"2025-07-18T08:43:38.932766Z","iopub.status.idle":"2025-07-18T18:17:01.753682Z","shell.execute_reply.started":"2025-07-18T08:43:38.932739Z","shell.execute_reply":"2025-07-18T18:17:01.751499Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 1 - Train Loss: 0.1568\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 2 - Train Loss: 0.0814\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 3 - Train Loss: 0.0520\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 4 - Train Loss: 0.0365\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 5 - Train Loss: 0.0172\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 6 - Train Loss: 0.0068\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 7 - Train Loss: 0.0291\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 8 - Train Loss: 0.0106\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 9 - Train Loss: 0.0011\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10 - Train Loss: 0.0006\n","output_type":"stream"},{"name":"stderr","text":"Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [03:47<00:00, 11.39s/it]","output_type":"stream"},{"name":"stdout","text":"Evaluation Results for 'Category 5':\nAccuracy:  0.9775\nPrecision: 0.8667\nRecall:    0.8966\nF1 Score:  0.8814\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":11}]}